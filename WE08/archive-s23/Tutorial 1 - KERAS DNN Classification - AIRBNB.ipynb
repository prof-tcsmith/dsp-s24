{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1 - KERAS DNN Classification\n",
    "\n",
    "We will predict the price category, among 4 categories, of an AIRBNB listing (`price_category` column). This is a multi-class classification task.\n",
    "\n",
    "**The unit of analysis is an AIRBNB LISTING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will predict the \"price_gte_150\" value in the data set:\n",
    "\n",
    "airbnb = pd.read_csv(\"airbnb.csv\")\n",
    "airbnb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(airbnb, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be careful: we haven't seperated the target column yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the variables we can't use in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't use the following columns in this tutorial\n",
    "\n",
    "train = train_set.drop(['price', 'price_gte_150'], axis=1)\n",
    "test = test_set.drop(['price', 'price_gte_150'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate the target variable (we don't want to transform it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train[['price_category']]\n",
    "test_target = test[['price_category']]\n",
    "\n",
    "train_inputs = train.drop(['price_category'], axis=1)\n",
    "test_inputs = test.drop(['price_category'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: Let's derive a new column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember, the \"minimum_nights\" column is highly skewed. Let's try to transform it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs['minimum_nights'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs['minimum_nights'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import power transformer from sklearn. It will help us create a \"normal distribution\"\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "PT = PowerTransformer(method = 'yeo-johnson', standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_min_nights = PT.fit_transform(train_inputs[['minimum_nights']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(transformed_min_nights).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE1: We didn't make the transformed variable as part of the input variables yet. To do that, we will use the pipeline.\n",
    "\n",
    "NOTE2: We don't need to create a function (like before). This transformer already has fit() and transform(). So, we can use this in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Identify the numerical and categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this stage, you can manually identify numeric, binary, and categorical columns as follows:**\n",
    "\n",
    "`numeric_columns = ['latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'Number of amenities', 'guests_included', 'price_per_extra_person', 'minimum_nights', 'number_of_reviews', 'number_days_btw_first_last_review', 'review_scores_rating']`\n",
    " \n",
    " `binary_columns = ['host_is_superhost', 'host_identity_verified']`\n",
    " \n",
    " `categorical_columns = ['neighbourhood_cleansed', 'property_type', 'room_type', 'bed_type', 'cancellation_policy']`\n",
    " \n",
    "<br>\n",
    " \n",
    "**If you do not want to manually type these, you can do the below tricks:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the numerical columns\n",
    "numeric_columns = train_inputs.select_dtypes(include=[np.number]).columns.to_list()\n",
    "\n",
    "# Identify the categorical columns\n",
    "categorical_columns = train_inputs.select_dtypes('object').columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the binary columns so we can pass them through without transforming\n",
    "binary_columns = ['host_is_superhost', 'host_identity_verified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful: numerical columns already includes the binary columns,\n",
    "# So, we need to remove the binary columns from numerical columns.\n",
    "\n",
    "for col in binary_columns:\n",
    "    numeric_columns.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_columns = ['minimum_nights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_column = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('powertransformer', PowerTransformer(method = 'yeo-johnson', standardize=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns),\n",
    "        ('binary', binary_transformer, binary_columns),\n",
    "        ('trans', my_new_column, transformed_columns)],\n",
    "        remainder='passthrough')\n",
    "\n",
    "#passtrough is an optional step. You don't have to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform: fit_transform() for TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and transform the train data\n",
    "train_x = preprocessor.fit_transform(train_inputs)\n",
    "\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tranform: transform() for TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data\n",
    "test_x = preprocessor.transform(test_inputs)\n",
    "\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras needs Ordinal target values for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ord_enc = OrdinalEncoder()\n",
    "\n",
    "train_y = ord_enc.fit_transform(train_target)\n",
    "\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = ord_enc.transform(test_target)\n",
    "\n",
    "test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "dummy_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Train Accuracy\n",
    "dummy_train_pred = dummy_clf.predict(train_x)\n",
    "\n",
    "baseline_train_acc = accuracy_score(train_y, dummy_train_pred)\n",
    "\n",
    "print('Baseline Train Accuracy: {}' .format(baseline_train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Test Accuracy\n",
    "dummy_test_pred = dummy_clf.predict(test_x)\n",
    "\n",
    "baseline_test_acc = accuracy_score(test_y, dummy_test_pred)\n",
    "\n",
    "print('Baseline Test Accuracy: {}' .format(baseline_test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification using Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is your input shape?\n",
    "#(meaning: how many neurons should be in the input layer?)\n",
    "\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer (Shallow) Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model: for multi-class\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Input(shape=67))\n",
    "model.add(keras.layers.Dense(50, activation='relu'))\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "#final layer: there has to be 4 nodes with softmax (because we have 4 categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "#Optimizer:\n",
    "adam = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal target (as in this example):\n",
    "\n",
    "Final layer's activation = **softmax** <br>\n",
    "loss = **sparse_categorical_crossentropy**\n",
    "\n",
    "## Binary target \n",
    "\n",
    "Final layer has only 1 neuron <br>\n",
    "Final layer's activation = **sigmoid** <br>\n",
    "loss = **binary_crossentropy**\n",
    "\n",
    "## One-hot target (rare cases)\n",
    "\n",
    "Final layer's activation = **softmax** <br>\n",
    "loss = **categorical_crossentropy**\n",
    "\n",
    "## Regression task (target is continuous)\n",
    "\n",
    "Final layer has only 1 neuron (keras.layers.Dense(1))<br>\n",
    "Activation is None<br>\n",
    "loss = **mean_squared_error**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "\n",
    "history = model.fit(train_x, train_y, \n",
    "                    validation_data=(test_x, test_y), \n",
    "                    epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "\n",
    "scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "\n",
    "scores\n",
    "\n",
    "# In results, first is loss, second is accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the accuracy from model.evaluate\n",
    "\n",
    "print(\"%s: %.2f\" % (model.metrics_names[0], scores[0]))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Network (Pipe Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model: for multi-class\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Input(shape=67))\n",
    "model.add(keras.layers.Dense(67, activation='relu'))\n",
    "model.add(keras.layers.Dense(67, activation='relu'))\n",
    "model.add(keras.layers.Dense(67, activation='relu'))\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "#final layer: there has to be 4 nodes with softmax (because we have 4 categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "#Optimizer:\n",
    "adam = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "\n",
    "history = model.fit(train_x, train_y, \n",
    "                    validation_data=(test_x, test_y), \n",
    "                    epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "\n",
    "scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "\n",
    "scores\n",
    "\n",
    "# In results, first is loss, second is accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the accuracy from model.evaluate\n",
    "\n",
    "print(\"%s: %.2f\" % (model.metrics_names[0], scores[0]))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide & Deep Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's send all inputs to the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "\n",
    "inputlayer = keras.layers.Input(shape=67)\n",
    "\n",
    "hidden1 = keras.layers.Dense(67, activation='relu')(inputlayer)\n",
    "hidden2 = keras.layers.Dense(67, activation='relu')(hidden1)\n",
    "hidden3 = keras.layers.Dense(67, activation='relu')(hidden2)\n",
    "\n",
    "concat = keras.layers.Concatenate()([inputlayer, hidden3])\n",
    "\n",
    "#final layer: there has to be 4 nodes with softmax (because we have 4 categories)\n",
    "output = keras.layers.Dense(4, activation='softmax')(concat)\n",
    "\n",
    "model = keras.Model(inputs =[inputlayer], outputs = output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "#Optimizer:\n",
    "adam = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "\n",
    "history = model.fit(train_x, train_y, \n",
    "                    validation_data=(test_x, test_y), \n",
    "                    epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "\n",
    "scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "\n",
    "scores\n",
    "\n",
    "# In results, first is loss, second is accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the accuracy from model.evaluate\n",
    "\n",
    "print(\"%s: %.2f\" % (model.metrics_names[0], scores[0]))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's send two inputs to the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first two columns: longitude and latitude\n",
    "#(WHY: because lat and lon are good and important predictors)\n",
    "\n",
    "lon_lat = train_x[:,:2]\n",
    "\n",
    "lon_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "input1 = keras.layers.Input(shape=2)\n",
    "input2 = keras.layers.Input(shape=67)\n",
    "\n",
    "hidden1 = keras.layers.Dense(67, activation='relu')(input2)\n",
    "hidden2 = keras.layers.Dense(67, activation='relu')(hidden1)\n",
    "hidden3 = keras.layers.Dense(67, activation='relu')(hidden2)\n",
    "\n",
    "concat = keras.layers.Concatenate()([input1, hidden3])\n",
    "\n",
    "#final layer: there has to be 4 nodes with softmax (because we have 4 categories)\n",
    "output = keras.layers.Dense(4, activation='softmax')(concat)\n",
    "\n",
    "model = keras.Model(inputs =[input1, input2], outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "#Optimizer:\n",
    "adam = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "\n",
    "history = model.fit((lon_lat, train_x), train_y, \n",
    "                    validation_data=((test_x[:,:2], test_x), test_y), \n",
    "                    epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "\n",
    "scores = model.evaluate((test_x[:,:2], test_x), test_y, verbose=0)\n",
    "\n",
    "scores\n",
    "\n",
    "# In results, first is loss, second is accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the accuracy from model.evaluate\n",
    "\n",
    "print(\"%s: %.2f\" % (model.metrics_names[0], scores[0]))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers, Learning rate, Dropout, Initialization & Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model: for multi-class\n",
    "\n",
    "\n",
    "#Set the learning rate:\n",
    "lr=0.001\n",
    "\n",
    "\n",
    "#Available optimizers:\n",
    "adagrad = keras.optimizers.Adagrad(learning_rate=lr, epsilon=None, decay=0.0)\n",
    "sgd = keras.optimizers.SGD(learning_rate=lr, momentum=0.0, decay=0.0, nesterov=False)\n",
    "rmsprop = keras.optimizers.RMSprop(learning_rate=lr, rho=0.9, epsilon=None, decay=0.0)\n",
    "adam = keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "nesterov_adam = keras.optimizers.Nadam(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "#Initializations:\n",
    "xavier = keras.initializers.glorot_normal(seed=None)\n",
    "he = keras.initializers.he_normal(seed=None)\n",
    "\n",
    "\n",
    "# Activation functions. Uncomment only one\n",
    "activation = 'elu' \n",
    "#activation = 'relu'\n",
    "#activation = 'tanh'\n",
    "#activation = 'sigmoid'\n",
    "\n",
    "\n",
    "\n",
    "#See the droput layers below:\n",
    "input1 = keras.layers.Input(shape=67)\n",
    "\n",
    "hidden1 = keras.layers.Dense(67, activation=activation, kernel_initializer=xavier)(input1)\n",
    "drop1   = keras.layers.Dropout(0.2)(hidden1)\n",
    "hidden2 = keras.layers.Dense(67, activation=activation, kernel_initializer=xavier)(drop1)\n",
    "drop2   = keras.layers.Dropout(0.2)(hidden2)\n",
    "hidden3 = keras.layers.Dense(67, activation=activation, kernel_initializer=xavier)(drop2)\n",
    "\n",
    "#final layer: there has to be 4 nodes with softmax (because we have 4 categories)\n",
    "output = keras.layers.Dense(4, activation='softmax')(hidden3)\n",
    "\n",
    "#Compile\"\n",
    "model = keras.Model(inputs = input1, outputs = output)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=nesterov_adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "\n",
    "history = model.fit(train_x, train_y, \n",
    "                    validation_data=(test_x, test_y), \n",
    "                    epochs=20, batch_size=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "\n",
    "scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "\n",
    "scores\n",
    "\n",
    "# In results, first is loss, second is accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the accuracy from model.evaluate\n",
    "\n",
    "print(\"%s: %.2f\" % (model.metrics_names[0], scores[0]))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping based on validation results\n",
    "\n",
    "To do this, you need to send the validation data sets to the fit() function and use a callback.\n",
    "\n",
    "EarlyStopping Arguments:\n",
    "\n",
    "**monitor:** quantity to be monitored.<br>\n",
    "**min_delta:** minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.<br>\n",
    "**patience:** number of epochs with no improvement after which training will be stopped.<br>\n",
    "**verbose:** verbosity mode.<br>\n",
    "**mode:** one of {auto, min, max}. In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity.<br>\n",
    "**baseline:** Baseline value for the monitored quantity to reach. Training will stop if the model doesn't show improvement over the baseline.<br>\n",
    "**restore_best_weights:** whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model: for multi-class\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Input(shape=67))\n",
    "model.add(keras.layers.Dense(67, activation='relu'))\n",
    "model.add(keras.layers.Dense(67, activation='relu'))\n",
    "model.add(keras.layers.Dense(67, activation='relu'))\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "#final layer: there has to be 4 nodes with softmax (because we have 4 categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=nesterov_adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "callback = [earlystop]\n",
    "\n",
    "model.fit(train_x, train_y, validation_data=(test_x, test_y), \n",
    "          epochs=100, batch_size=100, callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# If you don't have Sci-Keras installed, you need to convert this cell to \"code\" and execute it.\n",
    "\n",
    "!pip install scikeras[tensorflow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hidden_layer_sizes, dropout):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape=67))\n",
    "    for hidden_layer_size in hidden_layer_sizes:\n",
    "        model.add(keras.layers.Dense(hidden_layer_size, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dropout(dropout))\n",
    "    model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "\n",
    "keras_clf = KerasClassifier(\n",
    "    model=build_model,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    optimizer__learning_rate=0.01,\n",
    "    hidden_layer_sizes=(50,),\n",
    "    dropout=0,\n",
    "    verbose=False,\n",
    "    batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "callback = [earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {\n",
    "    'optimizer__learning_rate': [0.05, 0.1],\n",
    "    'model__hidden_layer_sizes': [(100, ), (50, 50, ), (67, 67, 67)],\n",
    "    'model__dropout': [0, 0.1],\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_clf, params, scoring='accuracy', n_iter=5, cv=3)\n",
    "\n",
    "rnd_search_cv.fit(train_x, train_y, epochs=5, validation_data=(test_x, test_y), \n",
    "                  callbacks=callback, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "\n",
    "train_preds = rnd_search_cv.best_estimator_.predict(train_x)\n",
    "\n",
    "test_preds = rnd_search_cv.best_estimator_.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"TRAIN:\")\n",
    "\n",
    "print(\"%s: %.2f\" % (\"Accuracy\", accuracy_score(train_preds, train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST:\")\n",
    "\n",
    "print(\"%s: %.2f\" % (\"Accuracy\", accuracy_score(test_preds, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
