{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - Text Mining - Classification - KERAS\n",
    "\n",
    "We will predict the category of discussion posts in a newsgroup.\n",
    "\n",
    "**The unit of analysis is a discussion post**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the target variable to ordinal\n",
    "\n",
    "This is a multi-class classification problem. There are three categories we will predict:<br>\n",
    "Whether a post is \"graphics,\" \"hockey,\" or \"medical\" related\n",
    "\n",
    "#### Keras doesn't like text-based target values. So, we have to change it to \"ordinal\" values. Though, this is only needed to convert each category to an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the target to ordinal\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "\n",
    "news['target'] = enc.fit_transform(news[['newsgroup']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = news['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign the \"text\" (input) variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "\n",
    "news[['TEXT']].isna().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# If there were missing values:\n",
    "\n",
    "news['TEXT'].fillna('missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = news['TEXT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set, train_y, test_y = train_test_split(input_data, target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras: Tokenizer\n",
    "\n",
    "In order to use Keras, you first need to install tensorflow. You can start the Anaconda Prompt and enter the following to do so: `pip install tensorflow`\n",
    "\n",
    "Keras Tokenizer works a little different than scikit-learn (but the idea is the same)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Convert this cell to \"code\" and execute it if you don't have tensorflow already installed\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tokenizer (after installing Tensorflow)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# When initializing a tokenizer, \"num_words\" selects the most frequently occuring N terms only\n",
    "# If you make it \"num_words=None\" then all terms are included\n",
    "keras_tokenizer = Tokenizer(num_words=500, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True)\n",
    "\n",
    "keras_tokenizer.fit_on_texts(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After identifying the terms to be used in the term-by-document matrix, \n",
    "# create the matrix using one of the below\n",
    "\n",
    "\n",
    "#train_binary_matrix = keras_tokenizer.texts_to_matrix(train_set, mode='binary')\n",
    "train_binary_matrix = keras_tokenizer.texts_to_matrix(train_set, mode='tfidf')\n",
    "#train_binary_matrix = keras_tokenizer.texts_to_matrix(train_set, mode='freq')  # ratio of terms in a document\n",
    "#train_binary_matrix = keras_tokenizer.texts_to_matrix(train_set, mode='count')\n",
    "\n",
    "\n",
    "train_binary_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to perform the test data set\n",
    "\n",
    "#test_binary_matrix = keras_tokenizer.texts_to_matrix(test_set, mode='binary')\n",
    "test_binary_matrix = keras_tokenizer.texts_to_matrix(test_set, mode='tfidf')\n",
    "#test_binary_matrix = keras_tokenizer.texts_to_matrix(test_set, mode='freq')  # ratio of terms in a document\n",
    "#test_binary_matrix = keras_tokenizer.texts_to_matrix(test_set, mode='count')\n",
    "\n",
    "\n",
    "test_binary_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_binary_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer's attributes:\n",
    "\n",
    "print(keras_tokenizer.word_counts)\n",
    "#print(keras_tokenizer.document_count)\n",
    "#print(keras_tokenizer.word_index)\n",
    "#print(keras_tokenizer.word_docs)\n",
    "\n",
    "\n",
    "# word_counts: A dictionary of words and their counts.\n",
    "# word_docs: A dictionary of words and how many documents each appeared in.\n",
    "# word_index: A dictionary of words and their uniquely assigned integers.\n",
    "# document_count:An integer count of the total number of documents that were used to fit the Tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are not creating SVDs here. But you can if you want. It is a matter of preference. \n",
    "### Also, note that we limited the columns to 500 (by selecting the most commonly occurring terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=300, n_jobs=-1) \n",
    "\n",
    "rnd_clf.fit(train_binary_matrix, train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train accuracy\n",
    "\n",
    "train_y_pred = rnd_clf.predict(train_binary_matrix)\n",
    "\n",
    "train_acc = accuracy_score(train_y, train_y_pred)\n",
    "\n",
    "print('Train acc: {}' .format(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test accuracy\n",
    "\n",
    "test_y_pred = rnd_clf.predict(test_binary_matrix)\n",
    "\n",
    "test_acc = accuracy_score(test_y, test_y_pred)\n",
    "\n",
    "print('Test acc: {}' .format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Usually created on test set\n",
    "confusion_matrix(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=100, tol=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.fit(train_binary_matrix, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train accuracy\n",
    "\n",
    "train_y_pred = sgd_clf.predict(train_binary_matrix)\n",
    "\n",
    "train_acc = accuracy_score(train_y, train_y_pred)\n",
    "\n",
    "print('Train acc: {}' .format(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test accuracy\n",
    "\n",
    "test_y_pred = sgd_clf.predict(test_binary_matrix)\n",
    "\n",
    "test_acc = accuracy_score(test_y, test_y_pred)\n",
    "\n",
    "print('Test acc: {}' .format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Usually created on test set\n",
    "confusion_matrix(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
