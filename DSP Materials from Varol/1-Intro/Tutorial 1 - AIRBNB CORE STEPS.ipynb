{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1 - AIRBNB - CORE STEPS\n",
    "\n",
    "**Our unit of analysis is an AIRBNB LISTING**\n",
    "\n",
    "We will see how we can transform the input variables. We won't do any predictions in this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will predict the \"median_house_value\" value in the data set:\n",
    "\n",
    "airbnb = pd.read_csv(\"airbnb.csv\")\n",
    "airbnb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the total number of rows\n",
    "\n",
    "airbnb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the missing values\n",
    "\n",
    "airbnb.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should we remove these rows or not???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to remove them, use the following code:\n",
    "\n",
    "# train.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(airbnb, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Descriptive statistics of numerical variables\n",
    "\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total missing values in each column\n",
    "\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate the POTENTIAL target columns. Separate numerical and categorical inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = train[['price', 'price_gte_150', 'price_category']]\n",
    "\n",
    "train_numeric_columns = train[['latitude', 'longitude', 'accommodates', \n",
    "                   'bathrooms', 'bedrooms', 'beds', 'Number of amenities', \n",
    "                   'guests_included', 'price_per_extra_person', 'minimum_nights', \n",
    "                   'number_of_reviews', 'number_days_btw_first_last_review', \n",
    "                   'review_scores_rating']]\n",
    "\n",
    "train_binary_columns = train[['host_is_superhost', 'host_identity_verified']]\n",
    "\n",
    "train_categorical_columns = train[['neighbourhood_cleansed', 'property_type', \n",
    "                                   'room_type', 'bed_type', 'cancellation_policy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numeric_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_binary_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categorical_columns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numeric_columns_imputed = imputer.fit_transform(train_numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_numeric_columns_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_numeric_columns_std = scaler.fit_transform(train_numeric_columns_imputed)\n",
    "\n",
    "train_numeric_columns_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert back to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numeric_columns_std_df = pd.DataFrame(train_numeric_columns_std, \n",
    "                                      columns=train_numeric_columns.columns).reset_index(drop=True)\n",
    "\n",
    "train_numeric_columns_std_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_numeric_columns_std_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the total number of missing values\n",
    "train_categorical_columns.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categorical_columns['property_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the rows that have missing values\n",
    "train_categorical_columns[train_categorical_columns.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute \"unknown\" or for categorical text values\n",
    "\n",
    "categorical_imputer = SimpleImputer(strategy=\"constant\", fill_value='UNKNOWN')\n",
    "\n",
    "train_categorical_columns_imputed = categorical_imputer.fit_transform(train_categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert back to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categorical_columns_imputed_df = pd.DataFrame(train_categorical_columns_imputed, \n",
    "                                      columns=train_categorical_columns.columns).reset_index(drop=True)\n",
    "\n",
    "train_categorical_columns_imputed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categorical_columns_imputed_df['property_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding\n",
    "Now let's preprocess the categorical variables using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "\n",
    "train_categorical_columns_1hot = cat_encoder.fit_transform(train_categorical_columns_imputed_df)\n",
    "\n",
    "train_categorical_columns_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `OneHotEncoder` class returns a sparse array, but we can convert it to a dense array if needed by calling the `toarray()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categorical_columns_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's flatten the array of arrays to get the column names\n",
    "\n",
    "onehot_column_names = [item for sublist in cat_encoder.categories_ for item in sublist]\n",
    "\n",
    "onehot_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert back to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categorical_columns_1hot_df = pd.DataFrame(train_categorical_columns_1hot.toarray(), \n",
    "                                           columns = onehot_column_names).reset_index(drop=True)\n",
    "\n",
    "train_categorical_columns_1hot_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not process the binary variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatanete these variables to the existing data set:\n",
    "# add reset_index(drop=True), otherwise, it adds NaN rows\n",
    "\n",
    "train_prepared = pd.concat((train_numeric_columns_std_df.reset_index(drop=True), \n",
    "                             train_categorical_columns_1hot_df.reset_index(drop=True),\n",
    "                             train_binary_columns.reset_index(drop=True)), axis=1)\n",
    "\n",
    "# if you want to create a separate column for missing values, use dummy_na=True:\n",
    "# pd.get_dummies(df,dummy_na=True)\n",
    "\n",
    "train_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prepared.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the Test data using \"Transform\" only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets = test[['price', 'price_gte_150', 'price_category']]\n",
    "\n",
    "test_numeric_columns = test[['latitude', 'longitude', 'accommodates', \n",
    "                   'bathrooms', 'bedrooms', 'beds', 'Number of amenities', \n",
    "                   'guests_included', 'price_per_extra_person', 'minimum_nights', \n",
    "                   'number_of_reviews', 'number_days_btw_first_last_review', \n",
    "                   'review_scores_rating']]\n",
    "\n",
    "test_binary_columns = test[['host_is_superhost', 'host_identity_verified']]\n",
    "\n",
    "test_categorical_columns = test[['neighbourhood_cleansed', 'property_type', \n",
    "                                 'room_type', 'bed_type', 'cancellation_policy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process numerical variables - test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform only\n",
    "\n",
    "test_numeric_columns_imputed = imputer.transform(test_numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_numeric_columns_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_numeric_columns_std = scaler.transform(test_numeric_columns_imputed)\n",
    "\n",
    "test_numeric_columns_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert back to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_numeric_columns_std_df = pd.DataFrame(test_numeric_columns_std, \n",
    "                                      columns=test_numeric_columns.columns).reset_index(drop=True)\n",
    "\n",
    "test_numeric_columns_std_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_numeric_columns_std_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the categorical variables - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the total number of missing values\n",
    "test_categorical_columns.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute \"unknown\" or for categorical text values\n",
    "\n",
    "test_categorical_columns_imputed = categorical_imputer.transform(test_categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert back to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_categorical_columns_imputed_df = pd.DataFrame(test_categorical_columns_imputed, \n",
    "                                      columns=test_categorical_columns.columns).reset_index(drop=True)\n",
    "\n",
    "test_categorical_columns_imputed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_categorical_columns_imputed_df['property_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding\n",
    "Now let's preprocess the categorical variables using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_categorical_columns_1hot = cat_encoder.transform(test_categorical_columns_imputed_df)\n",
    "\n",
    "test_categorical_columns_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `OneHotEncoder` class returns a sparse array, but we can convert it to a dense array if needed by calling the `toarray()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_categorical_columns_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot column names are still the same\n",
    "\n",
    "onehot_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert back to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_categorical_columns_1hot_df = pd.DataFrame(test_categorical_columns_1hot.toarray(), \n",
    "                                           columns = onehot_column_names).reset_index(drop=True)\n",
    "\n",
    "test_categorical_columns_1hot_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not transform the binary variables - test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate all variables - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatanete these variables to the existing data set:\n",
    "# add reset_index(drop=True), otherwise, it adds NaN rows\n",
    "\n",
    "test_prepared = pd.concat((test_numeric_columns_std_df.reset_index(drop=True), \n",
    "                           test_categorical_columns_1hot_df.reset_index(drop=True),\n",
    "                           test_binary_columns.reset_index(drop=True)), axis=1)\n",
    "\n",
    "# if you want to create a separate column for missing values, use dummy_na=True:\n",
    "# pd.get_dummies(df,dummy_na=True)\n",
    "\n",
    "test_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prepared.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we didn't do:\n",
    "\n",
    "Visualization<br>\n",
    "Feature engineering<br>\n",
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "235px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
